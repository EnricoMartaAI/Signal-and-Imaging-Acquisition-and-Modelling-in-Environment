{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9610122,"sourceType":"datasetVersion","datasetId":5863876},{"sourceId":9668141,"sourceType":"datasetVersion","datasetId":5907781}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"colab":{"provenance":[]}},"nbformat_minor":0,"nbformat":4,"cells":[{"cell_type":"markdown","source":["# Libraries"],"metadata":{"id":"fpIat6YRon32"}},{"cell_type":"code","source":["pip install -U albumentations"],"metadata":{"trusted":true,"id":"9swdzdpXon36"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pip install segmentation-models-pytorch"],"metadata":{"trusted":true,"id":"DJBE52Jkon38"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import os\n","from sklearn.model_selection import train_test_split\n","from PIL import Image\n","import torch\n","from torch.utils.data import Dataset\n","from torchvision import transforms\n","from torch.utils.data import DataLoader\n","import albumentations as A\n","from albumentations.pytorch import ToTensorV2\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from tqdm import tqdm\n","import torch.optim as optim\n","from torch.optim import lr_scheduler\n","from torchinfo import summary\n","import cv2"],"metadata":{"execution":{"iopub.status.busy":"2024-10-31T17:02:23.528792Z","iopub.execute_input":"2024-10-31T17:02:23.529342Z","iopub.status.idle":"2024-10-31T17:02:31.692522Z","shell.execute_reply.started":"2024-10-31T17:02:23.529298Z","shell.execute_reply":"2024-10-31T17:02:31.691212Z"},"trusted":true,"id":"bod1CyhPon38"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Data Preparation"],"metadata":{"id":"UaVIWttlon39"}},{"cell_type":"code","source":["# Load the dictionary that contains the classes for semantic segmentation\n","df = pd.read_csv('/kaggle/input/droneimages/class_dict_seg.csv')\n","\n","## Here is created a list to store the image paths and a list to store the masks\n","# Directories for the original images\n","dataset1_original_images_dir = '/kaggle/input/droneimages/Dataset1/Dataset/original_images'\n","dataset2_original_images_dir = '/kaggle/input/droneimages/Dataset2/Dataset/original_images'\n","\n","# Directories for the semantic label images\n","dataset1_label_images_semantic_dir = '/kaggle/input/droneimages/Dataset1/Dataset/label_images_semantic'\n","dataset2_label_images_semantic_dir = '/kaggle/input/droneimages/Dataset2/Dataset/label_images_semantic'\n","\n","# Create a list to store the image paths\n","image_paths = []\n","\n","# Add files from Dataset1\n","for root, dirs, files in os.walk(dataset1_original_images_dir):\n","    for file in files:\n","        if file.endswith(('jpg', 'png')):\n","            image_paths.append(os.path.join(root, file))\n","\n","# Add files from Dataset2\n","for root, dirs, files in os.walk(dataset2_original_images_dir):\n","    for file in files:\n","        if file.endswith(('jpg', 'png')):\n","            image_paths.append(os.path.join(root, file))\n","\n","# Convert the list into a DataFrame\n","df_images = pd.DataFrame(image_paths)\n","# Save the DataFrame to a CSV file\n","df_images.to_csv('/kaggle/working/image_paths.csv', index=False, header=False)\n","\n","# Same procedure to create a list to store the masks (= labels)\n","label_paths = []\n","\n","# Add label files from Dataset1\n","for root, dirs, files in os.walk(dataset1_label_images_semantic_dir):\n","    for file in files:\n","        if file.endswith(('jpg', 'png')):\n","            label_paths.append(os.path.join(root, file))\n","\n","# Add label files from Dataset2\n","for root, dirs, files in os.walk(dataset2_label_images_semantic_dir):\n","    for file in files:\n","        if file.endswith(('jpg', 'png')):\n","            label_paths.append(os.path.join(root, file))\n","\n","# Convert the list into a DataFrame\n","df_labels = pd.DataFrame(label_paths)\n","# Save the DataFrame to a CSV file\n","df_labels.to_csv('/kaggle/working/label_paths.csv', index=False, header=False)\n","\n","## Create class_mapping and colors_list from the dictionary\n","num_classes = len(df)\n","class_mapping = {}\n","colors_list = []\n","for idx, row in df.iterrows():\n","    class_name = row['name'] # class' name\n","    class_mapping[class_name] = idx  # class index\n","    rgb = [row[\" r\"], row[\" g\"], row[\" b\"]] #memorize the color associated to the class\n","    rgb_normalized = [c / 255.0 for c in rgb] #normalize it\n","    colors_list.append(rgb_normalized)\n","\n","## Here the aim is to create a unique structure to contain both the actual images and masks\n","# Extract just the file names\n","df_images['file_name'] = df_images[0].apply(lambda x: os.path.basename(x).split('.')[0])\n","df_labels['file_name'] = df_labels[0].apply(lambda x: os.path.basename(x).split('.')[0])\n","\n","# Merge based on the file names\n","data = pd.merge(df_images, df_labels, on='file_name')\n","\n","# Rename the columns based on the actual structure after merging\n","data.columns = ['image_path', 'file_name', 'label_path']\n","\n","# Drop the unnecessary 'file_name' column since we have already paired the images and labels\n","data = data[['image_path', 'label_path']]\n","\n","# Split the data into training set (90%) and validation set (10%)\n","train_data, val_data = train_test_split(data, test_size=0.10, random_state=42)\n","\n","# Save these splits into CSV files\n","train_data.to_csv('/kaggle/working/train_set.csv', index=False, header=False)\n","val_data.to_csv('/kaggle/working/val_set.csv', index=False, header=False)"],"metadata":{"execution":{"iopub.status.busy":"2024-10-31T17:02:31.695860Z","iopub.execute_input":"2024-10-31T17:02:31.696487Z","iopub.status.idle":"2024-10-31T17:02:31.913411Z","shell.execute_reply.started":"2024-10-31T17:02:31.696442Z","shell.execute_reply":"2024-10-31T17:02:31.911979Z"},"trusted":true,"id":"ZnR-4SBTon39"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Data Preprocessing"],"metadata":{"id":"FpdmNyR9on3-"}},{"cell_type":"code","source":["## The aim of this class is to load and prepare images and masks\n","## from the CSVs file for training the model\n","## It also manages the transformations\n","class DroneDataset(Dataset):\n","    def __init__(self, csv_file, transform=None):\n","        self.data = pd.read_csv(csv_file, header=None)\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        img_path = self.data.iloc[idx, 0]\n","        label_path = self.data.iloc[idx, 1]\n","\n","        image = np.array(Image.open(img_path).convert(\"RGB\"))\n","        mask = np.array(Image.open(label_path).convert(\"L\"))\n","\n","        if self.transform:\n","            augmented = self.transform(image=image, mask=mask)\n","            image = augmented['image']\n","            mask = augmented['mask']\n","        mask = mask.long()\n","\n","        return image, mask"],"metadata":{"execution":{"iopub.status.busy":"2024-10-31T17:02:31.915027Z","iopub.execute_input":"2024-10-31T17:02:31.915554Z","iopub.status.idle":"2024-10-31T17:02:31.924899Z","shell.execute_reply.started":"2024-10-31T17:02:31.915501Z","shell.execute_reply":"2024-10-31T17:02:31.923672Z"},"trusted":true,"id":"a7ToSEgSon3-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## Here is defined the training transformations for data augmentation, resizing, normalization\n","## and converting images and masks to tensors.\n","transform_train = A.Compose([\n","    A.Resize(128, 128),\n","    A.HorizontalFlip(p=0.5),\n","    A.VerticalFlip(p=0.5),\n","    A.RandomRotate90(p=0.5),\n","    A.RandomBrightnessContrast(p=0.4,brightness_limit=(-0.2, 0.2), contrast_limit=(-0.2, 0.2)),\n","    A.Perspective(scale=(0.02, 0.08), keep_size=True, p=0.3),\n","    A.Normalize(mean=(0.485, 0.456, 0.406),\n","                std=(0.229, 0.224, 0.225)),\n","    ToTensorV2()\n","])\n","\n","# No data augmentation for validation set\n","transform_val = A.Compose([\n","    A.Resize(128, 128),\n","    A.Normalize(mean=(0.485, 0.456, 0.406),\n","                std=(0.229, 0.224, 0.225)),\n","    ToTensorV2()\n","])"],"metadata":{"execution":{"iopub.status.busy":"2024-10-31T17:02:31.926522Z","iopub.execute_input":"2024-10-31T17:02:31.926940Z","iopub.status.idle":"2024-10-31T17:02:31.946255Z","shell.execute_reply.started":"2024-10-31T17:02:31.926900Z","shell.execute_reply":"2024-10-31T17:02:31.944969Z"},"trusted":true,"id":"s2Ozvwo8on3_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create the datasets\n","trainset = DroneDataset(csv_file='/kaggle/working/train_set.csv',\n","                        transform=transform_train)\n","\n","valset = DroneDataset(csv_file='/kaggle/working/val_set.csv',\n","                      transform=transform_val)\n","\n","batch_size = 20\n","\n","# Create the DataLoaders\n","trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=4,pin_memory=True,prefetch_factor=4)\n","valloader = DataLoader(valset, batch_size=1, shuffle=False, num_workers=4,pin_memory=True,prefetch_factor=4)"],"metadata":{"execution":{"iopub.status.busy":"2024-10-31T17:02:31.947495Z","iopub.execute_input":"2024-10-31T17:02:31.947856Z","iopub.status.idle":"2024-10-31T17:02:31.963714Z","shell.execute_reply.started":"2024-10-31T17:02:31.947808Z","shell.execute_reply":"2024-10-31T17:02:31.962383Z"},"trusted":true,"id":"eDzGse5son3_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# U-Net"],"metadata":{"id":"hnR2XVOPon3_"}},{"cell_type":"code","source":["class DoubleConv(nn.Module):\n","\n","    def __init__(self, in_channels, out_channels, mid_channels=None):\n","        super().__init__()\n","        if not mid_channels:\n","            mid_channels = out_channels\n","        self.double_conv = nn.Sequential(\n","            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),\n","            nn.BatchNorm2d(mid_channels),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),\n","            nn.BatchNorm2d(out_channels),\n","            nn.ReLU(inplace=True)\n","        )\n","\n","    def forward(self, x):\n","        return self.double_conv(x)"],"metadata":{"execution":{"iopub.status.busy":"2024-10-31T17:02:31.965322Z","iopub.execute_input":"2024-10-31T17:02:31.965810Z","iopub.status.idle":"2024-10-31T17:02:31.974922Z","shell.execute_reply.started":"2024-10-31T17:02:31.965758Z","shell.execute_reply":"2024-10-31T17:02:31.973716Z"},"trusted":true,"id":"uRqef-9Kon4A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Down(nn.Module):\n","    \"\"\"Downscaling with maxpool then double conv\"\"\"\n","\n","    def __init__(self, in_channels, out_channels):\n","        super().__init__()\n","        self.maxpool_conv = nn.Sequential(\n","            nn.MaxPool2d(2),\n","            DoubleConv(in_channels, out_channels)\n","        )\n","\n","    def forward(self, x):\n","        return self.maxpool_conv(x)\n"],"metadata":{"execution":{"iopub.status.busy":"2024-10-31T17:02:31.976295Z","iopub.execute_input":"2024-10-31T17:02:31.976648Z","iopub.status.idle":"2024-10-31T17:02:31.989260Z","shell.execute_reply.started":"2024-10-31T17:02:31.976612Z","shell.execute_reply":"2024-10-31T17:02:31.988013Z"},"trusted":true,"id":"ZhfTrnY9on4A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Up(nn.Module):\n","    \"\"\"Upscaling then double conv\"\"\"\n","\n","    def __init__(self, in_channels, out_channels, bilinear=True):\n","        super().__init__()\n","\n","        # if bilinear, use the normal convolutions to reduce the number of channels\n","        if bilinear:\n","            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n","            self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)\n","        else:\n","            self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n","            self.conv = DoubleConv(in_channels, out_channels)\n","\n","    def forward(self, x1, x2):\n","        x1 = self.up(x1)\n","        # input is CHW\n","        diffY = x2.size()[2] - x1.size()[2]\n","        diffX = x2.size()[3] - x1.size()[3]\n","\n","        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n","                        diffY // 2, diffY - diffY // 2])\n","        # if you have padding issues, see\n","        # https://github.com/HaiyongJiang/U-Net-Pytorch-Unstructured-Buggy/commit/0e854509c2cea854e247a9c615f175f76fbb2e3a\n","        # https://github.com/xiaopeng-liao/Pytorch-UNet/commit/8ebac70e633bac59fc22bb5195e513d5832fb3bd\n","        x = torch.cat([x2, x1], dim=1)\n","        return self.conv(x)\n"],"metadata":{"execution":{"iopub.status.busy":"2024-10-31T17:02:31.993172Z","iopub.execute_input":"2024-10-31T17:02:31.993640Z","iopub.status.idle":"2024-10-31T17:02:32.003562Z","shell.execute_reply.started":"2024-10-31T17:02:31.993600Z","shell.execute_reply":"2024-10-31T17:02:32.002368Z"},"trusted":true,"id":"CZHI7tw-on4A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class OutConv(nn.Module):\n","    def __init__(self, in_channels, out_channels):\n","        super(OutConv, self).__init__()\n","        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n","\n","    def forward(self, x):\n","        return self.conv(x)\n"],"metadata":{"execution":{"iopub.status.busy":"2024-10-31T17:02:32.005491Z","iopub.execute_input":"2024-10-31T17:02:32.005875Z","iopub.status.idle":"2024-10-31T17:02:32.027947Z","shell.execute_reply.started":"2024-10-31T17:02:32.005836Z","shell.execute_reply":"2024-10-31T17:02:32.026798Z"},"trusted":true,"id":"DbWaTorHon4A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class UNet(nn.Module):\n","    def __init__(self, n_channels, n_classes, bilinear=False):\n","        super(UNet, self).__init__()\n","        self.n_channels = n_channels\n","        self.n_classes = n_classes\n","        self.bilinear = bilinear\n","\n","        self.inc = (DoubleConv(n_channels, 64))\n","        self.down1 = (Down(64, 128))\n","        self.down2 = (Down(128, 256))\n","        self.down3 = (Down(256, 512))\n","        factor = 2 if bilinear else 1\n","        self.down4 = (Down(512, 1024 // factor))\n","        self.up1 = (Up(1024, 512 // factor, bilinear))\n","        self.up2 = (Up(512, 256 // factor, bilinear))\n","        self.up3 = (Up(256, 128 // factor, bilinear))\n","        self.up4 = (Up(128, 64, bilinear))\n","        self.outc = (OutConv(64, n_classes))\n","\n","    def forward(self, x):\n","        x1 = self.inc(x)\n","        x2 = self.down1(x1)\n","        x3 = self.down2(x2)\n","        x4 = self.down3(x3)\n","        x5 = self.down4(x4)\n","        x = self.up1(x5, x4)\n","        x = self.up2(x, x3)\n","        x = self.up3(x, x2)\n","        x = self.up4(x, x1)\n","        logits = self.outc(x)\n","        return logits"],"metadata":{"execution":{"iopub.status.busy":"2024-10-31T17:02:32.029302Z","iopub.execute_input":"2024-10-31T17:02:32.029649Z","iopub.status.idle":"2024-10-31T17:02:32.042704Z","shell.execute_reply.started":"2024-10-31T17:02:32.029615Z","shell.execute_reply":"2024-10-31T17:02:32.041365Z"},"trusted":true,"id":"DIZrvDtson4A"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Ensemble Method"],"metadata":{"id":"iBJbgwgBon4B"}},{"cell_type":"code","source":["class EnsembleModel(nn.Module):\n","    def __init__(self, models):\n","        super(EnsembleModel, self).__init__()\n","        self.models = nn.ModuleList(models)\n","\n","    def forward(self, x):\n","        # Prediction of each model\n","        outputs = [model(x) for model in self.models]\n","        # mean\n","        mean_output = torch.mean(torch.stack(outputs), dim=0)\n","        return mean_output"],"metadata":{"execution":{"iopub.status.busy":"2024-10-31T17:02:32.044126Z","iopub.execute_input":"2024-10-31T17:02:32.045386Z","iopub.status.idle":"2024-10-31T17:02:32.059510Z","shell.execute_reply.started":"2024-10-31T17:02:32.045328Z","shell.execute_reply":"2024-10-31T17:02:32.058212Z"},"trusted":true,"id":"bwme84Wgon4B"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Model"],"metadata":{"id":"V6zkpegpon4B"}},{"cell_type":"code","source":["# Define the number of classes (= 24)\n","num_classes = df.shape[0]\n","\n","# Initialize the model\n","model = UNet(n_channels=3, n_classes=num_classes, bilinear=False)\n","\n","## Here is possible to define a U-Net Model with Transfer Learning\n","\"\"\"\n","model = smp.Unet(\n","    encoder_name=\"mobilenet_v2\",\n","    encoder_weights=\"imagenet\",\n","    in_channels=3,\n","    classes=num_classes,\n",")\n","\"\"\"\n","\n","## Here is possible to define the Ensemble method\n","\"\"\"\n","res_unet = smp.Unet(encoder_name=\"resnet18\", encoder_weights=\"imagenet\", classes=num_classes, activation=None)\n","effnet = smp.Unet(encoder_name=\"efficientnet-b3\", encoder_weights=\"imagenet\", classes=num_classes, activation=None)\n","vggnet = smp.Unet(encoder_name=\"vgg11\", encoder_weights=\"imagenet\", classes=num_classes, activation=None)\n","model = EnsembleModel([res_unet, effnet, vggnet])\n","\"\"\"\n","\n","###  Here is possible to upload the weights for Self Supervised Learning\n","\"\"\"\n","# Load the pretrained weights\n","pretrained_dict = torch.load('/kaggle/input/unetweightsssl/unet_model_weights.pth', map_location=device, weights_only=True)\n","\n","# Important to remove 'module.' prefix from keys\n","pretrained_dict = {k.replace('module.', ''): v for k, v in pretrained_dict.items()}\n","\n","# Get the current model state dict\n","model_dict = model.state_dict()\n","\n","# Only update the matching keys from the pretrained dict\n","for k, v in pretrained_dict.items():\n","    if k in model_dict:\n","        model_dict[k] = v\n","        print(f\"Weights for layer '{k}' have been loaded.\")\n","    else:\n","        print(f\"Skipping layer '{k}' because it wasn't in the pretext task's UNet.\")\n","\n","# Load the updated state dict into the model\n","model.load_state_dict(model_dict)\n","\"\"\"\n","\n","# Define the loss function\n","criterion = nn.CrossEntropyLoss()\n","\n","## Here is possible to set as loss the IoU Loss\n","#criterion = smp.losses.JaccardLoss(mode='multiclass')\n","\n","# Define the optimizer\n","optimizer = optim.Adam(model.parameters(), lr=1e-3)\n","\n","# Define the learning rate scheduler\n","scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5)"],"metadata":{"execution":{"iopub.status.busy":"2024-10-31T17:02:32.061656Z","iopub.execute_input":"2024-10-31T17:02:32.062195Z","iopub.status.idle":"2024-10-31T17:02:32.496678Z","shell.execute_reply.started":"2024-10-31T17:02:32.062126Z","shell.execute_reply":"2024-10-31T17:02:32.495363Z"},"trusted":true,"id":"YQQcJSsbon4B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Check if CUDA is available and count the number of GPUs\n","if torch.cuda.is_available():\n","    num_gpus = torch.cuda.device_count()\n","    print(f'Number of GPUs available: {num_gpus}')\n","    if num_gpus < 2:\n","        print(\"There are less than 2 GPUs detected.\")\n","    device = torch.device('cuda:0')\n","else:\n","    device = torch.device('cpu')\n","    print('GPU is not available. Using CPU.')"],"metadata":{"trusted":true,"id":"5bkVKQCeon4B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## If multiple GPUs are available\n","if torch.cuda.is_available() and torch.cuda.device_count() > 1:\n","    print(\"Using DataParallel for multi-GPU training.\")\n","    model = nn.DataParallel(model, device_ids=[0, 1])\n","\n","## Move the model to device\n","model.to(device)\n","print(f'Model is using device: {device}')\n","\n","if isinstance(model, nn.DataParallel):\n","    print(f'Model is parallelized on devices: {model.device_ids}')"],"metadata":{"trusted":true,"id":"10METhDRon4B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## Calculate pixel-wise accuracy\n","def calculate_accuracy(preds, masks):\n","    correct = (preds == masks).float()\n","    acc = correct.sum() / correct.numel()\n","    return acc.item()\n"],"metadata":{"execution":{"iopub.status.busy":"2024-10-31T17:02:32.533618Z","iopub.execute_input":"2024-10-31T17:02:32.534025Z","iopub.status.idle":"2024-10-31T17:02:32.540517Z","shell.execute_reply.started":"2024-10-31T17:02:32.533987Z","shell.execute_reply":"2024-10-31T17:02:32.539314Z"},"trusted":true,"id":"Hy2QWrVaon4C"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## Calculate the IoU for each class and return the mean of it\n","def calculate_iou(preds, masks, num_classes):\n","\n","    ious = []\n","\n","    # 1D arrays\n","    preds = preds.view(-1)\n","    masks = masks.view(-1)\n","\n","    # For each class\n","    for cls in range(num_classes):\n","        # TRUE where predictions and masks match the current class, otherwise FALSE\n","        pred_inds = preds == cls\n","        target_inds = masks == cls\n","\n","        # Intersection: both prediction and mask match the current class\n","        intersection = (pred_inds[target_inds]).long().sum().item()\n","\n","        # Union: either prediction or mask matches the current class (Intersection must be considered once!!)\n","        union = pred_inds.long().sum().item() + target_inds.long().sum().item() - intersection\n","\n","        # Avoid division for zero\n","        if union == 0:\n","            ious.append(float('nan'))\n","        else:\n","            # IoU = intersection / union\n","            ious.append(float(intersection) / float(max(union, 1)))\n","\n","    # Ignore the NaN values\n","    ious = [iou for iou in ious if not np.isnan(iou)]\n","\n","    return np.mean(ious[:-1]) if len(ious) > 0 else float('nan')\n"],"metadata":{"execution":{"iopub.status.busy":"2024-10-31T17:02:32.541990Z","iopub.execute_input":"2024-10-31T17:02:32.542412Z","iopub.status.idle":"2024-10-31T17:02:32.553418Z","shell.execute_reply.started":"2024-10-31T17:02:32.542349Z","shell.execute_reply":"2024-10-31T17:02:32.552245Z"},"trusted":true,"id":"qCYs9FKGon4C"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Training"],"metadata":{"id":"xg0KKIKFon4C"}},{"cell_type":"code","source":["## Training\n","\n","# Lists for recording training and validation data\n","train_losses = []\n","val_losses = []\n","\n","train_ious = []\n","val_ious = []\n","\n","train_accuracies = []\n","val_accuracies = []\n","\n","# Training parameters\n","num_epochs = 150\n","# Start with no biased value\n","best_val_loss = float('inf')\n","# Number of epochs without improvement before stopping training\n","patience = 10\n","# Counter for early stopping\n","epochs_no_improve = 0\n","# Boolean for triggering early stopping\n","early_stop = False\n","\n","for epoch in range(num_epochs):\n","    if early_stop:\n","        print(\"Early stopping activated. Stopping training.\")\n","        break\n","\n","    print(f'Epoch {epoch+1}/{num_epochs}')\n","    print('-' * 10)\n","\n","    # Training Phase\n","    model.train()\n","    running_loss = 0.0\n","    running_iou = 0.0\n","    running_accuracy = 0.0\n","    for images, masks in tqdm(trainloader, desc='Training'):\n","        images = images.to(device)\n","        masks = masks.to(device)\n","\n","        # Zero the gradients\n","        optimizer.zero_grad()\n","\n","        # Forward pass\n","        outputs = model(images)\n","\n","        # Calculate loss\n","        loss = criterion(outputs, masks)\n","\n","        # Backward pass and optimization\n","        loss.backward()\n","        optimizer.step()\n","\n","        running_loss += loss.item()\n","        _, preds = torch.max(outputs, dim=1)\n","        # Calculate IoU\n","        iou = calculate_iou(preds, masks, num_classes)\n","        running_iou += iou\n","        # Calculate Pixel-Wise Accuracy\n","        accuracy = calculate_accuracy(preds, masks)\n","        running_accuracy += accuracy\n","\n","    epoch_loss = running_loss / len(trainloader)\n","    epoch_iou = running_iou / len(trainloader)\n","    epoch_accuracy = running_accuracy / len(trainloader)\n","    train_losses.append(epoch_loss)\n","    train_ious.append(epoch_iou)\n","    train_accuracies.append(epoch_accuracy)\n","    print(f'Training - Loss: {epoch_loss:.4f}, IoU: {epoch_iou:.4f}, Accuracy: {epoch_accuracy:.4f}')\n","\n","\n","    # Validation Phase\n","    model.eval()\n","    val_running_loss = 0.0\n","    val_running_iou = 0.0\n","    val_running_accuracy = 0.0\n","    with torch.no_grad():\n","        for images, masks in tqdm(valloader, desc='Validation'):\n","            images = images.to(device)\n","            masks = masks.to(device)\n","\n","            # Forward pass\n","            outputs = model(images)\n","\n","            # Calculate loss\n","            loss = criterion(outputs, masks)\n","            val_running_loss += loss.item()\n","\n","            _, preds = torch.max(outputs, dim=1)\n","            # Calculate IoU\n","            iou = calculate_iou(preds, masks, num_classes)\n","            val_running_iou += iou\n","            # Calculate Pixel-Wise Accuracy\n","            accuracy = calculate_accuracy(preds, masks)\n","            val_running_accuracy += accuracy\n","\n","    val_epoch_loss = val_running_loss / len(valloader)\n","    val_epoch_iou = val_running_iou / len(valloader)\n","    val_epoch_accuracy = val_running_accuracy / len(valloader)\n","    val_losses.append(val_epoch_loss)\n","    val_ious.append(val_epoch_iou)\n","    val_accuracies.append(val_epoch_accuracy)\n","\n","    print(f'Validation - Loss: {val_epoch_loss:.4f}, IoU: {val_epoch_iou:.4f}, Accuracy: {val_epoch_accuracy:.4f}')\n","\n","    # Update the scheduler with the validation loss\n","    scheduler.step(val_epoch_loss)\n","\n","    # Early Stopping Phase\n","    if val_epoch_loss < best_val_loss:\n","        best_val_loss = val_epoch_loss\n","        epochs_no_improve = 0\n","        torch.save(model.state_dict(), 'best_unet_model.pth')\n","        print('Model saved!')\n","    else:\n","        epochs_no_improve += 1\n","        print(f'Validation loss did not improve for {epochs_no_improve} epoch(s).')\n","\n","    # Early Stopping\n","    if epochs_no_improve >= patience:\n","        print(f'Early stopping activated. No improvement in validation loss for {patience} consecutive epochs.')\n","        early_stop = True\n","\n","    print()"],"metadata":{"trusted":true,"id":"xw7F9mcOon4C"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Actual number of epochs\n","actual_epochs = len(train_losses)\n","\n","epochs = range(1, actual_epochs + 1)\n","plt.figure(figsize=(18, 6))\n","\n","# Loss Plot\n","plt.subplot(1, 3, 1)\n","plt.plot(epochs, train_losses, 'b-', label='Training Loss')\n","plt.plot(epochs, val_losses, 'r-', label='Validation Loss')\n","plt.title('Loss per Epoch')\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","plt.legend()\n","plt.grid(True)\n","\n","# IoU Plot\n","plt.subplot(1, 3, 2)\n","plt.plot(epochs, train_ious, 'b-', label='Training IoU')\n","plt.plot(epochs, val_ious, 'r-', label='Validation IoU')\n","plt.title('IoU per Epoch')\n","plt.xlabel('Epoch')\n","plt.ylabel('IoU')\n","plt.legend()\n","plt.grid(True)\n","\n","# Accuracy Plot\n","plt.subplot(1, 3, 3)\n","plt.plot(epochs, train_accuracies, 'b-', label='Training Accuracy')\n","plt.plot(epochs, val_accuracies, 'r-', label='Validation Accuracy')\n","plt.title('Accuracy per Epoch')\n","plt.xlabel('Epoch')\n","plt.ylabel('Accuracy')\n","plt.legend()\n","plt.grid(True)\n","\n","plt.tight_layout()\n","plt.show()\n","\n","plt.savefig('training_validation_metrics.png')\n"],"metadata":{"execution":{"iopub.status.busy":"2024-10-31T17:22:43.674600Z","iopub.status.idle":"2024-10-31T17:22:43.675359Z","shell.execute_reply.started":"2024-10-31T17:22:43.674975Z","shell.execute_reply":"2024-10-31T17:22:43.675009Z"},"trusted":true,"id":"EelazsMRon4C"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Landing and Visualitation"],"metadata":{"id":"Ur57UZ7qon4D"}},{"cell_type":"code","source":["# Function that manages both the visualitation of the predicted mask and determines the landing point for each image\n","def visualize_and_landing_point(model, dataloader, device, num_classes, colors_list, class_mapping, non_sensitive_classes, output_dir):\n","\n","    if not os.path.exists(output_dir):\n","        os.makedirs(output_dir)\n","\n","    # Assigns colors to each class in the mask\n","    def decode_mask(mask, colors_list):\n","        color_mask = np.zeros((mask.shape[0], mask.shape[1], 3))\n","        for cls in range(len(colors_list)):\n","            color_mask[mask == cls] = colors_list[cls]\n","        return color_mask\n","\n","    all_classes = set(class_mapping.keys())\n","    non_sensitive_set = set(non_sensitive_classes)\n","    sensitive_classes = list(all_classes - non_sensitive_set)\n","\n","    # For each image in the validation set: the landing point\n","    for idx, (images, masks) in enumerate(dataloader):\n","\n","        image = images[0].to(device)\n","        mask = masks[0].to(device)\n","\n","        image_batch = image.unsqueeze(0)\n","\n","        with torch.no_grad():\n","            output = model(image_batch)\n","\n","        _, preds = torch.max(output, dim=1)\n","\n","        image_np = image.cpu().numpy().transpose(1, 2, 0)\n","        mask_np = mask.cpu().numpy()\n","        preds_np = preds.squeeze().cpu().numpy()\n","\n","        mean = np.array([0.485, 0.456, 0.406])\n","        std = np.array([0.229, 0.224, 0.225])\n","        # the original image is recreated\n","        image_np = std * image_np + mean\n","        image_np = np.clip(image_np, 0, 1)\n","\n","        # the right colors\n","        mask_color = decode_mask(mask_np, colors_list)\n","        preds_color = decode_mask(preds_np, colors_list)\n","\n","        # from names to idx\n","        sensitive_indices = [class_mapping[cls] for cls in sensitive_classes]\n","        # create the mask where 1 sensitive, 0 not-sensitive\n","        sensitive_mask = np.isin(preds_np, sensitive_indices).astype(np.uint8)\n","\n","\n","\n","        # Edges are sensible as well\n","        border_thickness = 3\n","        H, W = preds_np.shape\n","        sensitive_mask[0:border_thickness, :] = 1\n","        sensitive_mask[H - border_thickness:H, :] = 1\n","        sensitive_mask[:, 0:border_thickness] = 1\n","        sensitive_mask[:, W - border_thickness:W] = 1\n","\n","\n","        inverted_mask = 1 - sensitive_mask\n","        # For each pixel with 1 (no sensitive) calculates the distance from the nearest pixel with 0 (sensitive)\n","        distance_map = cv2.distanceTransform(inverted_mask, distanceType=cv2.DIST_L2, maskSize=5)\n","\n","        # The maximum distance is\n","        max_dist = np.max(distance_map)\n","        # The coordinates of the pixel at maximum distance is\n","        max_pos = np.unravel_index(np.argmax(distance_map), distance_map.shape)\n","        print(f\"Image: {idx+1}/{len(dataloader)}, Landing Point: {max_pos}, Distance: {max_dist}\")\n","\n","        landing_point_image = image_np.copy()\n","        landing_point_image = (landing_point_image * 255).astype(np.uint8)\n","\n","        # \"X\" in red\n","        x_size = 5\n","        color_x = (255, 0, 0)\n","        thickness_x = 2\n","        x, y = max_pos[1], max_pos[0]\n","        cv2.line(landing_point_image_uint8, (x - x_size, y - x_size), (x + x_size, y + x_size), color_x, thickness=thickness_x)\n","        cv2.line(landing_point_image_uint8, (x - x_size, y + x_size), (x + x_size, y - x_size), color_x, thickness=thickness_x)\n","\n","\n","        fig, axs = plt.subplots(1, 4, figsize=(24, 6))\n","        axs[0].imshow(image_np)\n","        axs[0].set_title('Original Image')\n","        axs[0].axis('off')\n","\n","        axs[1].imshow(mask_color)\n","        axs[1].set_title('Mask')\n","        axs[1].axis('off')\n","\n","        axs[2].imshow(preds_color)\n","        axs[2].set_title('Prediction')\n","        axs[2].axis('off')\n","\n","        axs[3].imshow(landing_point_image)\n","        axs[3].set_title('Landing Point')\n","        axs[3].axis('off')\n","\n","        plt.tight_layout()\n","\n","        output_path = os.path.join(output_dir, f'prediction_{idx+1}.png')\n","        plt.savefig(output_path)\n","        plt.close(fig)\n","\n","    print(f\"The images are saved in {output_dir}\")"],"metadata":{"execution":{"iopub.status.busy":"2024-10-31T17:22:43.677807Z","iopub.status.idle":"2024-10-31T17:22:43.678458Z","shell.execute_reply.started":"2024-10-31T17:22:43.678121Z","shell.execute_reply":"2024-10-31T17:22:43.678181Z"},"trusted":true,"id":"G3SQ3Ayuon4D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.load_state_dict(torch.load('/kaggle/working/best_unet_model.pth'))\n","model.eval()\n","\n","non_sensitive_classes = ['grass','paved-area']\n","output_directory = '/kaggle/working/OutputImages'\n","\n","visualize_and_landing_point(\n","    model=model,\n","    dataloader=valloader,\n","    device=device,\n","    num_classes=len(class_mapping),\n","    colors_list=colors_list,\n","    class_mapping=class_mapping,\n","    non_sensitive_classes=non_sensitive_classes,\n","    output_dir=output_directory\n",")"],"metadata":{"execution":{"iopub.status.busy":"2024-10-31T17:22:43.681029Z","iopub.status.idle":"2024-10-31T17:22:43.681554Z","shell.execute_reply.started":"2024-10-31T17:22:43.681318Z","shell.execute_reply":"2024-10-31T17:22:43.681342Z"},"trusted":true,"id":"oM6b79V4on4D"},"execution_count":null,"outputs":[]}]}