{"cells":[{"cell_type":"markdown","metadata":{"id":"R9mmfMTZKYvB"},"source":["# Libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"6cKyB0SMs2sv"},"outputs":[],"source":["import numpy as np\n","from astropy.io import fits\n","import scipy.ndimage as ndi\n","import matplotlib.pyplot as mp\n","from google.colab import drive\n","import glob\n","\n","mp.rcParams['figure.figsize'] = (15, 10)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"fs3lslaDtRRN"},"outputs":[],"source":["drive.mount('/content/drive/', force_remount=True)\n","path = \"/content/drive/MyDrive/Colab Notebooks/Enviornment/L6/Telescope Data/Imaging Science/\""]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"itCIS-zIynRe"},"outputs":[],"source":["!ls \"/content/drive/MyDrive/Colab Notebooks/Enviornment/L6/Telescope Data/Imaging Science/\""]},{"cell_type":"markdown","metadata":{"id":"FQokrAmfKdB7"},"source":["# Code"]},{"cell_type":"markdown","metadata":{"id":"kuG-hTUbz7Wo"},"source":["Process the bias and dark frames, then generate a master dark frame for 300s exposure by extrapolating from existing shorter exposure darks. This extrapolation accounts for the increase in dark current with exposure time, allowing estimation of a 300s dark frame even if direct measurements at that exposure are unavailable.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"OE64H_hw1buv"},"outputs":[],"source":["files = glob.glob(path + 'calib*bias*')  # Find all calibration files that match the pattern 'calib*bias*' (bias frames).\n","Nfiles = len(files)  # Count the number of bias files found.\n","print('Number of frames:{}'.format(Nfiles))  # Print the number of bias frames.\n","\n","hdu = fits.open(files[0])  # Open the first bias frame file.\n","bias0 = hdu[0].data  # Extract the data from the FITS file (image array).\n","hdu.close()  # Close the FITS file to free resources.\n","\n","ny, nx = np.shape(bias0)  # Get the dimensions (ny: rows, nx: columns) of the bias frame.\n","\n","allbias = np.zeros((Nfiles, ny, nx))  # Initialize an empty array to store all bias frames.\n","\n","for ind, ff in enumerate(files):\n","    hdu = fits.open(ff)  # Open each bias frame file.\n","    allbias[ind, ...] = hdu[0].data  # Store the bias frame data in the 'allbias' array.\n","    hdu.close()  # Close the FITS file to free resources.\n","\n","masterbias = np.median(allbias, axis=0)  # Compute the master bias frame by taking the median of all bias frames.\n","fits.writeto('masterbias.fits', masterbias, overwrite=True)  # Save the master bias frame to a new FITS file.\n","del allbias  # Delete the 'allbias' array to free memory after use.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"4nGqnm73LaTY"},"outputs":[],"source":["def process_dark_frames(path, exposure_time):\n","    # Find all dark frames that match the specified exposure time in the directory.\n","    files = glob.glob(path + f'calib*dark*{exposure_time}*')\n","    Nfiles = len(files)  # Count the number of dark frames found.\n","    print(f'Number of {exposure_time}s dark frames: {Nfiles}')  # Output the number of dark frames found.\n","\n","    if Nfiles > 0:  # If there are any dark frames, process them.\n","        hdu = fits.open(files[0])  # Open the first dark frame file.\n","        dark0 = hdu[0].data  # Extract the data (image array) from the FITS file.\n","        hdu.close()  # Close the FITS file to free resources.\n","\n","        ny, nx = np.shape(dark0)  # Get the dimensions (ny: rows, nx: columns) of the dark frame.\n","        all_dark = np.zeros((Nfiles, ny, nx))  # Initialize an array to store all dark frames.\n","\n","        # Loop over all dark frame files and load them into the 'all_dark' array.\n","        for ind, ff in enumerate(files):\n","            hdu = fits.open(ff)\n","            all_dark[ind, ...] = hdu[0].data  # Store the dark frame data at the corresponding index.\n","            hdu.close()  # Close each FITS file to free resources.\n","\n","        # Calculate the master dark frame by taking the median across all frames.\n","        masterdark = np.median(all_dark, axis=0)\n","        del all_dark  # Delete the 'all_dark' array to free memory.\n","\n","        # Save the master dark frame to a FITS file.\n","        fits.writeto(f'masterdark_{exposure_time}.fits', masterdark, overwrite=True)\n","        print(f'Master dark frame for {exposure_time}s saved as masterdark_{exposure_time}.fits')\n","\n","    else:\n","        # If no dark frames were found, print a message indicating that.\n","        print(f'No dark frames found for {exposure_time}s.')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"TpEnRPlMLzqZ"},"outputs":[],"source":["process_dark_frames(path, 60)\n","process_dark_frames(path, 120)"]},{"cell_type":"markdown","metadata":{"id":"fzWeCIZuxJU_"},"source":["Instead of taking additional dark frames with 300-second exposures (which consumes more time and resources), you can extrapolate the dark frame using the relationship between dark current and time."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"ektq_hWVh9PK"},"outputs":[],"source":["# Load the master bias frame and the master dark frame for 120s exposure time.\n","masterbias = fits.getdata('masterbias.fits')\n","masterdark_120 = fits.getdata('masterdark_120.fits')\n","\n","# Generate the master dark frame for 300s by extrapolating from the 120s dark frame.\n","# The equation scales the thermal signal (dark current) from 120s to 300s.\n","# It assumes the dark current is linear with exposure time:\n","# (masterdark_120 - masterbias) isolates the thermal signal, scales it by (300/120), and adds back the bias.\n","masterdark_300 = ((masterdark_120 - masterbias) / 120 * 300) + masterbias\n","\n","# Save the resulting 300s master dark frame to a FITS file.\n","fits.writeto('masterdark_300.fits', masterdark_300, overwrite=True)\n"]},{"cell_type":"markdown","metadata":{"id":"BMVTgxUC4FIl"},"source":["Now combine the flat frames after removing the bias. Since flats are short exposures, the dark current contribution is negligible. The primary CCD calibration needed here is bias subtraction to remove the readout noise.This ensures that the pixel sensitivity variations in the flat frames are measured accurately without contamination from the bias."]},{"cell_type":"markdown","metadata":{"id":"iZp3KofgLwK2"},"source":["An H-alpha filter is designed to let through only a narrow band of wavelengths around 656.28 nm, blocking out most other wavelengths. This is useful for isolating the light emitted by hydrogen in astronomical objects while reducing interference from other types of light (such as starlight or artificial light pollution).\n","\n","The OIII (Oxygen-III) filter isolates a narrow wavelength band around 500.7 nm, targeting light emitted by doubly ionized oxygen. This is commonly used to observe emission nebulae, particularly planetary nebulae and supernova remnants, revealing areas where oxygen is excited by intense radiation.\n","\n"," The SII (Sulfur-II) filter focuses on a narrow band around 672.4 nm, highlighting light emitted by singly ionized sulfur. It helps in identifying regions of ionized sulfur in star-forming areas and supernova remnants."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"tJI9-ZDc1JVt"},"outputs":[],"source":["def load_and_stack_flats(filter_name, masterbias, path):\n","    # Load flat frames for the specified filter (OIII, SII, etc.) using the provided filter name.\n","    files = glob.glob(path + f'flat*{filter_name}*')  # Find all flat files matching the filter name.\n","    Nfiles = len(files)  # Count the number of flat frames found.\n","    print(f'Number of {filter_name} flat frames: {Nfiles}')  # Print the number of frames.\n","\n","    if Nfiles > 0:  # If there are flat frames, proceed with the stacking process.\n","        hdu = fits.open(files[0])  # Open the first flat frame file.\n","        flat0 = hdu[0].data  # Extract the data from the FITS file (image array).\n","        hdu.close()  # Close the FITS file to free resources.\n","\n","        ny, nx = np.shape(flat0)  # Get the dimensions (ny: rows, nx: columns) of the flat frame.\n","        all_flats = np.zeros((Nfiles, ny, nx))  # Initialize an array to store all flat frames.\n","\n","        # Loop over each flat frame, load it, subtract the bias, and store the corrected frame.\n","        for ind, ff in enumerate(files):\n","            hdu = fits.open(ff)\n","            flat = hdu[0].data  # Extract the flat frame data.\n","            hdu.close()\n","\n","            # Subtract the master bias from each flat frame to remove bias noise.\n","            flat_corrected = flat - masterbias\n","            all_flats[ind, ...] = flat_corrected  # Store the corrected flat frame.\n","\n","        # Stack the corrected flat frames by taking the median to create the master flat frame.\n","        master_flat = np.median(all_flats, axis=0)\n","\n","        # Normalize the master flat by dividing by its median value to ensure uniform illumination correction.\n","        master_flat /= np.median(master_flat)\n","\n","        # Save the master flat frame as a FITS file for later use.\n","        fits.writeto(f'masterflat_{filter_name}.fits', master_flat, overwrite=True)\n","\n","        return master_flat  # Return the master flat frame for further use.\n","    else:\n","        # If no flat frames were found, print a message and return None.\n","        print(f\"No {filter_name} flat frames found.\")\n","        return None\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"5cnLkoCK1LNW"},"outputs":[],"source":["masterbias = fits.getdata('masterbias.fits')\n","masterdark_60 = fits.getdata('masterdark_60.fits')\n","masterdark_120 = fits.getdata('masterdark_120.fits')\n","masterdark_300 = fits.getdata('masterdark_300.fits')\n","\n","# Repeat for H filter\n","master_flat_Halpha = load_and_stack_flats('Halpha', masterbias, path)\n","\n","# Stack the OIII flat frames to create a master flat\n","master_flat_OIII = load_and_stack_flats('OIII', masterbias, path)\n","\n","# Repeat for SII filter\n","master_flat_SII = load_and_stack_flats('SII', masterbias, path)"]},{"cell_type":"markdown","metadata":{"id":"WPy0EkFIkPqs"},"source":["# Halpha"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"Wnb1XQu2K8h6"},"outputs":[],"source":["num = ['001', '002', '003', '004', '005', '006']  # List of identifiers corresponding to the science frames for different observations (H-alpha, OIII, SII).\n","exptime = [60, 60, 120, 120, 300, 300]  # Corresponding exposure times (in seconds) for each science frame. Frames '001' and '002' have 60s exposures, '003' and '004' have 120s, and '005' and '006' have 300s.\n","Nscience = len(num)  # Number of science frames, which corresponds to the length of the 'num' list.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"2sk9VUKk2yrc"},"outputs":[],"source":["filefmt = path + 'NGC956_{}Halpha_{}s.fit'  # Define the file format for the science frames, using placeholders for frame number and exposure time.\n","halpha_reduced = np.zeros((Nscience, ny, nx))  # Initialize an array to store the calibrated H-alpha frames for all science frames.\n","\n","# CCD gain (conversion factor from ADU to electrons), given in e-/ADU.\n","ccd_gain = 0.6  # Replace with your camera's actual gain.\n","\n","for ind, nn in enumerate(num):  # Loop over all science frames.\n","    exp_time = exptime[ind]  # Get the exposure time for the current frame.\n","\n","    # Select the appropriate dark frame based on the exposure time.\n","    if exp_time == 60:\n","        dark = masterdark_60\n","    elif exp_time == 120:\n","        dark = masterdark_120\n","    else:\n","        dark = masterdark_300\n","\n","    # Open the corresponding H-alpha science frame.\n","    hdu = fits.open(filefmt.format(num[ind], exp_time))\n","    science_frame = hdu[0].data  # Extract the science frame data.\n","    hdu.close()  # Close the FITS file to free resources.\n","\n","    # Apply bias and dark subtraction\n","    calibrated_frame = (science_frame - masterbias - dark)\n","\n","    # Apply flat field correction by dividing by the master flat\n","    flat_corrected_frame = calibrated_frame / master_flat_Halpha\n","\n","    # Normalize by gain and exposure time, final units will be e-/s\n","    halpha_reduced[ind, ...] = (flat_corrected_frame / ccd_gain) / exp_time\n","\n","# The resulting 'halpha_reduced' array contains the fully calibrated H-alpha science frames in e-/s.\n"]},{"cell_type":"markdown","metadata":{"id":"9RMjgkpa6KvP"},"source":["Perform a co-add of the frames using a mean to preserve the signal-to-noise ratio (SNR).\n","Frames are weighted by the inverse variance to give more importance to less noisy frames.\n","Variance is proportional to the inverse of the square root of the exposure time, meaning frames with longer exposure times have lower variance and thus higher weights. This ensures that higher-quality frames contribute more to the final co-added result.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"uMiP8F0U_8cq"},"outputs":[],"source":["# Perform a weighted average of the H-alpha frames, using weights based on exposure time.\n","# The weights are calculated as (1 - 1/exptime), giving more importance to longer exposure times.\n","# This ensures that frames with higher signal-to-noise ratio (SNR) have a greater influence on the co-added result.\n","halpha_coadd = np.average(halpha_reduced, weights=np.array(exptime), axis=0)\n","\n","# Optionally save the co-added frame as a FITS file.\n","hdu = fits.PrimaryHDU(halpha_coadd)\n","hdu.writeto('/content/COADD1.fits', overwrite=True)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"n2E7nExwACCO"},"outputs":[],"source":["# Display the co-added H-alpha frame using a color scale between 10 and 50, with the origin set to the lower-left corner for correct orientation.\n","mp.imshow(halpha_coadd, vmin=10, vmax=50, origin='lower')\n","# Set the y-axis limits to zoom into the region between pixel rows 2000 and 2080.\n","mp.ylim(2000, 2080)\n","# Set the x-axis limits to zoom into the region between pixel columns 2090 and 2170.\n","mp.xlim(2090, 2170)"]},{"cell_type":"markdown","metadata":{"id":"nqZNkRIbr9z9"},"source":["Due to significant star movement between exposures, image registration is necessary before co-adding. This is done by selecting a reference star and fitting its profile to find the centroid in each exposure. Once the centroids are determined, the images can be shifted to align the star in the same physical position, ensuring proper alignment of all exposures before co-addition. This prevents misalignment artifacts and improves the final image quality.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"9QRYsUjC6nK9"},"outputs":[],"source":["!pip install photutils  # Install the 'photutils' package, used for astronomical image processing, including tasks like centroiding and aperture photometry.\n","\n","!pip install sep  # Install the 'sep' package, which provides functions for source extraction and photometry, commonly used in astronomical data analysis.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"-SJtYEaS-dr2"},"outputs":[],"source":["import photutils.centroids as cent  # Import the centroiding functions from the 'photutils.centroids' module, which provides methods for calculating the centroid of stars and other sources in astronomical images.\n"]},{"cell_type":"markdown","metadata":{"id":"j7F648L3VNvX"},"source":["The centroid_quadratic method is a technique used to find the centroid (the \"center of mass\") of a star or object in an astronomical image by fitting a quadratic function to the pixel values."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"L_GSU-PD-rNE"},"outputs":[],"source":["xcent = []\n","ycent = []\n","\n","# Loop over all science frames to find the star's centroid in each frame\n","for i in range(Nscience):\n","    # Calculate the centroid using the 'centroid_quadratic' method on a subregion around the star\n","    xycent = cent.centroid_quadratic(halpha_reduced[i, 2010:2050, 2110:2150])\n","\n","    # Append the centroid coordinates adjusted for the full image position\n","    xcent.append(xycent[0] + 2110)  # Adjust for the x-offset of the subregion\n","    ycent.append(xycent[1] + 2010)  # Adjust for the y-offset of the subregion\n","\n","# Convert the lists to NumPy arrays for easier calculations\n","xcent = np.array(xcent)\n","ycent = np.array(ycent)\n","\n","# Calculate the offsets in x and y relative to the first image's centroid\n","xoff = xcent[0] - xcent\n","yoff = ycent[0] - ycent\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"09AtZO0-AZju"},"outputs":[],"source":["xoff, yoff"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"_5Rn8bjz_byy"},"outputs":[],"source":["fig, axes = mp.subplots(nrows=2, ncols=3, figsize=(8,5))  # Create a 2x3 grid of subplots with a figure size of 8x5 inches.\n","\n","# Loop through each subplot and corresponding image region.\n","for ind, ax in enumerate(axes.flat):\n","    try:\n","        # Display a subregion of the H-alpha image (rows 2010:2050, columns 2110:2150) with pixel intensity limits.\n","        ax.imshow(halpha_reduced[ind, 2010:2050, 2110:2150], vmin=10, vmax=50, origin='lower')\n","\n","        # Plot the calculated star centroid in the subregion as a red cross.\n","        ax.scatter([xcent[ind] - 2110], [ycent[ind] - 2010], marker='+', color='red')\n","\n","    # If there are fewer images than subplots, pass without error.\n","    except:\n","        pass\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"0eMXlxL8BFTu"},"outputs":[],"source":["# Align the H-alpha frames by shifting them based on the calculated offsets from the centroids.\n","# The shift function moves the image by (yoff, xoff), ensuring that the star is aligned across all frames.\n","for i in range(1, Nscience):\n","    halpha_reduced[i, ...] = ndi.shift(halpha_reduced[i, ...], (yoff[i], xoff[i]))\n"]},{"cell_type":"markdown","metadata":{"id":"0IwwJOaEJkdK"},"source":["Interpolating the data with splines can lead to artifacts as splines can overshoot the data, you can try np.roll to shift the data. Roll does not interpolate the input but you are limited to integer pixel shifts"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"77ELXZMgEDTN"},"outputs":[],"source":["fig, axes = mp.subplots(nrows=2, ncols=3, figsize=(8, 5))  # Create a 2x3 grid of subplots for displaying image regions, with a figure size of 8x5 inches.\n","\n","# Loop through each subplot and corresponding image region.\n","for ind, ax in enumerate(axes.flat):\n","    try:\n","        # Display a subregion of the H-alpha image (rows 2010:2050, columns 2110:2150) with pixel intensity limits.\n","        ax.imshow(halpha_reduced[ind, 2010:2050, 2110:2150], vmin=10, vmax=50, origin='lower')\n","\n","        # Plot the centroid of the star in the first image as a red cross for reference alignment.\n","        ax.scatter([xcent[0] - 2110], [ycent[0] - 2010], marker='+', color='red')\n","\n","    # If there are fewer images than subplots, skip the remaining plots without error.\n","    except:\n","        pass\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"bBzAjvmnIzMv"},"outputs":[],"source":["#Coadd again the images\n","\n","# Variance is inversely proportional to exposure time\n","inverse_variance_weights = np.array(exptime)\n","\n","# Co-add the shifted images using inverse variance weighting\n","halpha_coadd = np.average(halpha_reduced, weights=inverse_variance_weights, axis=0)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"mqh-QuR_I8B5"},"outputs":[],"source":["mp.imshow(halpha_coadd,vmin=-1, vmax=5, origin='lower')\n","mp.ylim(1000,2000)\n","mp.xlim(1500,2500)"]},{"cell_type":"markdown","metadata":{"id":"kCkWm1Y_ecKT"},"source":["We now measure the flux in one of the *stars*\n","\n","To measure the flux of a star in the co-added image, we need to perform aperture photometry. This method involves summing the pixel values within a circular region (the aperture) around the star, while subtracting the background noise from the surrounding region.\n","\n","Select the star: Identify the star's centroid (which you might already have from earlier centroid calculations).\n","\n","Define an aperture: Use a circular aperture centered on the star.\n","\n","Subtract background: Measure the background level using an annulus (a ring around the aperture) and subtract it from the total flux.\n","\n","Compute flux: Sum the pixel values within the aperture and subtract the estimated background to get the star's total flux."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"PkPjuqWjBBZD"},"outputs":[],"source":["from photutils.aperture import aperture_photometry, CircularAnnulus, CircularAperture, ApertureStats\n","# Import aperture photometry tools from the 'photutils' package. This includes:\n","# - aperture_photometry: for performing photometry within defined apertures.\n","# - CircularAnnulus and CircularAperture: for defining circular regions and annuli around stars for photometry.\n","# - ApertureStats: for computing statistics within the defined apertures.\n","\n","from astropy.visualization import simple_norm\n","# Import 'simple_norm' from 'astropy.visualization' to easily normalize image data for better visualization.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"YBG7t1GPEp2S"},"outputs":[],"source":["#Define the source position and prepare a simple plot to identify the regions for data and background\n","positions = [(2132.2,2022.2)]\n","aperture = CircularAperture(positions, r=12)\n","annulus_aperture = CircularAnnulus(positions, r_in=15, r_out=20)\n","\n","norm = simple_norm(halpha_coadd, 'sqrt', percent=99)\n","mp.imshow(halpha_coadd, norm=norm, interpolation='nearest')\n","mp.ylim(1980,2060)\n","mp.xlim(2090,2170)\n","\n","ap_patches = aperture.plot(color='white', lw=2,label='Photometry aperture')\n","ann_patches = annulus_aperture.plot(color='red', lw=2,label='Background annulus')\n","\n","handles = (ap_patches[0], ann_patches[0])\n","mp.legend(loc=(0.17, 0.05), facecolor='#458989', labelcolor='white',\n","           handles=handles, prop={'weight': 'bold', 'size': 11})\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"NPO2FL9lHIYZ"},"outputs":[],"source":["phot = aperture_photometry(halpha_coadd, aperture)\n","print(phot)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"Kgu45ChdG6Ej"},"outputs":[],"source":["apflux = []\n","\n","for i in np.arange(1,25):\n","  aperture = CircularAperture(positions, r=i)\n","  apflux.append(aperture_photometry(halpha_coadd, aperture)['aperture_sum'])\n","\n","mp.plot(np.arange(1,25), apflux)\n","mp.ylabel('Flux')\n","mp.xlabel('Radius')\n","\n","#Is this what you can naively expect from the image above? If not, what could be wrong?"]},{"cell_type":"markdown","metadata":{"id":"ag7SrN439Q7F"},"source":["You are trying to measure the flux (brightness) of a star in an astronomical image. When doing this, there are two main challenges:\n","\n","Background Noise: The image contains not just the star’s light but also unwanted light from the background (e.g., from the sky, scattered light, or nearby stars). We need to estimate and subtract this background from the star’s total flux.\n","\n","Outliers in Background: The background might contain outliers like cosmic rays or noise spikes that could distort the background estimation. These need to be removed or \"clipped.\""]},{"cell_type":"markdown","metadata":{"id":"z3KrgcP098uv"},"source":["Solution Steps:\n","Aperture Photometry:\n","\n","Aperture photometry involves placing a circular region (the aperture) around the star and summing the pixel values inside this aperture to estimate the total flux.\n","However, this total flux includes both the star's light and the background noise.\n","Background Estimation with an Annulus:\n","\n","To estimate the background, we define an annulus (a ring-shaped region) around the star. This annulus is outside the main star’s light but still within the surrounding background.\n","The pixel values in this annulus represent the background light, which can be used to estimate the background flux inside the aperture.\n","Sigma Clipping to Handle Outliers:\n","\n","The background annulus may contain outliers such as bright pixels caused by cosmic rays, hot pixels, or other noise. These outliers would distort the background estimation.\n","Sigma clipping helps remove those outliers by iteratively rejecting pixels that deviate significantly (in this case, more than 3 standard deviations) from the median background level. This way, we get a cleaner estimate of the background.\n","Calculate the Star's Net Flux:\n","\n","Once we know the background level (from the annulus), we subtract the background contribution from the total flux measured in the aperture.\n","The result is the net flux, which represents the star’s brightness with the background noise subtracted."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"YwcYjdpaGVQe"},"outputs":[],"source":["from astropy.stats import SigmaClip\n","\n","#Redefine apertures for convenience\n","aperture = CircularAperture(positions, r=12)\n","annulus_aperture = CircularAnnulus(positions, r_in=15, r_out=20)\n","\n","#Define a clipping scheme\n","sigclip = SigmaClip(sigma=3.0, maxiters=10)\n","\n","#Do the photometry\n","aper_stats = ApertureStats(halpha_coadd, aperture, sigma_clip=None)\n","bkg_stats = ApertureStats(halpha_coadd, annulus_aperture, sigma_clip=sigclip)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"dFZ3Gp_IGqyI"},"outputs":[],"source":["#This mean is for one pixel\n","print(bkg_stats.mean)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"wne5JqgCGsjX"},"outputs":[],"source":["#This value is over N pixels\n","print(aper_stats.sum)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"ydl7qaLKG0d4"},"outputs":[],"source":["#The background needs to be scaled to the right number of pixels\n","total_bkg = bkg_stats.mean * aper_stats.sum_aper_area.value\n","\n","print(aper_stats.sum - total_bkg)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"3-2FpHsmIsR_"},"outputs":[],"source":["apflux = []\n","apflux_bkg = []\n","\n","for i in np.arange(1,25):\n","  aperture = CircularAperture(positions, r=i)\n","  apflux.append(aperture_photometry(halpha_coadd, aperture)['aperture_sum'])\n","  apflux_bkg.append(aperture_photometry(halpha_coadd, aperture)['aperture_sum']-bkg_stats.mean * aperture.area)\n","\n","mp.plot(np.arange(1,25), apflux)\n","mp.plot(np.arange(1,25), apflux_bkg)\n","mp.ylabel('Flux')\n","mp.xlabel('Radius')\n","\n","#Which of these two lines do you find to be more correct?"]}],"metadata":{"colab":{"provenance":[{"file_id":"1RSP5csGnKWblauMQwv8psxFFvmHsSMVo","timestamp":1727873824313}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}